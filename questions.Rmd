---
output:
  pdf_document: default
  html_document: default
---
```{r, libraries_question, include=FALSE}
library(tidyverse)
library(cowplot)
library(corrplot)
library(car)
library(FactoMineR)
library(lmtest)
library(xtable)
library(ggfortify)
library(ggrepel)
library(MASS)
library(mvoutlier)

select <- dplyr::select
recode <- dplyr::recode
some <- purrr::some
```

```{r helper_funcs, include=FALSE}
pval <- function(res) {
    pval <- res$p.value

    ifelse(pval < 2.2e-16,
           "less than $2.2\\times10^{-16}$",
           signif(pval, 2)
    )
}
```


# Analysis
```{r}
source("io_checksum.R")
df <- read_rds_w_checksum("./data/dataset.rds")
```


<!-- 1 -->
## Determine if the response variable (price) has an acceptably normal distribution. Address test to discard serial correlation.

```{r q1_price_hist, echo = FALSE, fig.height = 3}

price_mean <- mean(df$price)
price_sd <- sd(df$price)

ggplot(df, aes(price)) +
    geom_histogram(aes(y = ..density..), bins = 30) +
    stat_function(
        fun = dnorm,
        args = list(mean = price_mean, sd = price_sd),
        aes(color = "normal"),
    ) +
    geom_density(aes(color = "density"), linetype = "dashed") +
    geom_text(aes(
        x = max(df$price) * 3 / 4, y = 1e-5,
        color = "normal"
    ),
    label = sprintf(
        "x̄ = %0.2f\ns = %0.2f",
        price_mean,
        price_sd
    ),
    show.legend = FALSE,
    ) +
    guides(color = guide_legend(title = NULL))
```

```{r q1_price_log_hist, echo = FALSE, fig.height = 3}

price_log_mean <- mean(log(df$price))
price_log_sd <- sd(log(df$price))

ggplot(df, aes(log(price))) +
    geom_histogram(aes(y = ..density..), bins = 30) +
    stat_function(
        fun = dnorm,
        args = list(mean = price_log_mean, sd = price_log_sd),
        aes(color = "normal")
    ) +
    geom_density(aes(color = "density"), linetype = "dashed") +
    geom_text(aes(
        x = 11, y = 0.75,
        color = "normal"
    ),
    label = sprintf(
        "x̄ = %0.2f\ns = %0.2f",
        price_log_mean,
        price_log_sd
    ),
    show.legend = FALSE,
    ) +
    guides(color = guide_legend(title = NULL))
```

```{r q1_shapiro}
(shap_res <- shapiro.test(df$price))
(log_shap_res <- shapiro.test(log(df$price)))
```

The histogram of the price shows a very right skewed distribution which does not
seem compatible with a normal fit. Moreover, the Shapiro test returns a small
*p-value* of `r pval(shap_res)`, which makes us reject the null hypothesis
of normality.

Additionally, we also checked if the price followed a log normal distribution. If we
analyse the new histogram we can see that the log transformation corrected the skewness
and the new distribution seems to resemble a normal bell shape. However the Shapiro test
returns a *p-value* of `r pval(log_shap_res)`, which makes us
reject the null hypothesis of normality.

\Cref{fig:q1_qq_plots} shows the QQ plots of `price` and `log*(price)`. We can
see that `price` clearly does not follow a normal distribution and that
`log(price)` is heavy tailed.


```{r q1_qq_plots, echo=FALSE, fig.cap="QQ plots", fig.width=6}
plt_qq_price <- ggplot(df, aes(sample = price)) +
    stat_qq() +
    stat_qq_line(color = "red") +
    theme(aspect.ratio = 1) +
    labs(x = "Theoretical", y = "price")
plt_qq_log_price <- ggplot(df, aes(sample = log(price))) +
    stat_qq() +
    stat_qq_line(color = "red") +
    theme(aspect.ratio = 1) +
    labs(x = "Theoretical", y = "log(price)")

plot_grid(plt_qq_price, plt_qq_log_price, align = "hv", labels = "AUTO")
```

```{r}
dwtest_res <- dwtest(price ~ 1, data = df)
```

We perform a Durbin-Watson test with the null hypothesis that the
autocorrelation of the disturbances is 0. We obtain a *p-value* of
`r pval(dwtest_res)` so we fail to reject the null hypothesis.

```{r}
dwtest_res
```

The results of the test are consistent with the visual interpretation of
the ACF plot^[lag 0 is omitted for clarity] shown in \cref{fig:acf}.
All the values except lag = 33 lie within the confidence interval of 95%,
showing that there is no autocorrelation.

```{r acf, fig.cap = "ACF plot for price"}
acf_res <- acf(df$price, plot = FALSE)
acf_res_df <- with(acf_res, data.frame(lag, acf)) %>%
    slice(-1) # Remove lag 0

# Confidence interval
acf_ci <- function(acf_res, alpha = 0.05) {
    return(qnorm((1 + (1 - alpha)) / 2) / sqrt(acf_res$n.used))
}

conf_inter <- acf_ci(acf_res, 0.05)

ggplot(data = acf_res_df, mapping = aes(x = lag, y = acf)) +
    geom_hline(aes(yintercept = 0)) +
    geom_segment(mapping = aes(xend = lag, yend = 0)) +
    geom_hline(yintercept = c(conf_inter, -conf_inter), linetype = "dashed", color = "blue") +
    labs(x = "Lag", y = "ACF")
```

<!-- 2. --> \pagebreak
## Indicate by exploration of the data, which are apparently the variables most associated with the response variable (use only the indicated variables).

Since we determined that `price` does not follow a normal distribution, we
compute a correlation matrix using the `spearman` coefficient. The plot
of the correlation is shown in \cref{fig:q2_corrplot} and shows that the
numerical variables most associated with price are: `age`, `mileage` and
`mpg`. Surprisingly, `tax` has the lowest correlation coefficient. The
specific values of the correlation matrix are shown in \cref{tab:q2_corr}.

```{r q2_corrplot, fig.cap = "Spearman correlation plot", fig.height = 2.5}
cor_mat <- df %>%
    select(where(is.numeric)) %>%
    cor(use = "complete.obs", method = "spearman")

cor_mat %>% corrplot()
```

```{=latex }
\begin{multicols}{2}
```

```{r q2_corr}
cor_mat %>%
    kable(
        caption = "Spearman correlation coefficients",
        booktabs = TRUE,
        digits = 2
    ) %>%
    kable_styling(latex_options = c("HOLD_position"), full_width = FALSE)
```

```{r q2_factors}
df_cond <- df %>%
    select(-c(starts_with("aux_"))) %>%
    condes(which(colnames(.) == "price"))

df_cond$quali %>%
    data.frame() %>%
    rownames_to_column("Variable") %>%
    select(-p.value) %>%
    kable(
        caption = "Qualitative variable correlation with price",
        booktabs = TRUE,
        digits = 2
    ) %>%
    kable_styling(latex_options = c("HOLD_position"), full_width = FALSE)
```

```{=latex }
\end{multicols}
```

Using `condes` method from `FactoMineR`, we computed the correlation with the
qualitative variables as shown in \cref{tab:q2_factors}. The most relevant
qualitative variable is `model`, closely followed by `engineSize` and then
`year` (this correlates with the results of the numerical variable `age`).
Finally, `transmission` has little less significance and `manufacturer` and
`fuelType` have almost no significance.

The variables most associated with our response variable are (in decreasing
order of importance):

1. `model`
1. `engineSize`
1. `year` / `age`
1. `mileage`
1. `mpg`

<!-- 3. --> \pagebreak
## Define a polytomic factor f.age for the covariate car age according to its quartiles, and argue if the average price depends on the level of age.  Statistically justify the answer.

We start by checking the ANOVA assumptions of normality and homogeneity of
variance.

```{r anova_assumptions, fig.cap = "Boxplot of price by age group"}
df %>%
    ggplot(aes(x = aux_age, y = price, fill = aux_age)) +
    geom_boxplot()

# Homogenity of variance
flig_res <- fligner.test(price ~ aux_age, data = df); flig_res

# Normality
model <- lm(price ~ aux_age, data = df)
shapiro.test(residuals(model))
```
The fligner test returns a *p-value* of `r pval(flig_res)` which makes us reject the
null hypothesis of homoscedasticity. Additionally, we can't assume normality. For this
reason we will use the non-parametric Kruskal-Wallis test.

```{r q3_bp}
(kruskal_res <- kruskal.test(price ~ aux_age, data = df))
```
The test returns a *p.value* of `r pval(kruskal_res)`. Which is less than
the significance level and thus we reject the NULL hypothesis that the location
parameters of all the samples are equal. We have statistical proof that
the average `price` does depend on the `age`. The visual inspection of
the boxplots in \cref{fig:anova_assumptions} is consistent with the results.

<!-- 4. --> \pagebreak
## Calculate and interpret the anova model that explains car price according to the age factor and the fuel type.

```{r q4}
df %>%
    ggplot(aes(x = aux_age, y = price, fill = fuelType)) +
    geom_boxplot()

anova <- aov(price ~ aux_age * fuelType, data = df)
summary(anova)
```
Anova results show that the both factors are significant (_p.value < 0.05_) as
well as their interaction.

```{r q4_summary}
df %>%
    group_by(aux_age, fuelType) %>%
    summarise(
        count = n(),
        mean = mean(price),
        sd = sd(price),
        .groups = "keep"
    ) %>%
    kable(
        caption = "Summary of price by age and fuelType",
        booktabs = TRUE,
        digits = 2
    ) %>%
    kable_styling(latex_options = c("HOLD_position"), full_width = FALSE)

```


<!-- 5. --> \pagebreak
## Do you think that the variability of the price depends on both factors? Does the relation between price and age factor depend on fuel type?

```{r}
(fligner_price <- fligner.test(price ~ fuelType, data = df))
(fligner_age <- fligner.test(price ~ aux_age, data = df))
(fligner_int <- fligner.test(price ~ interaction(aux_age, fuelType), data = df))
```
We execute Fligner-Killeen test with each factor and the interaction of both.
The resulting *p.values* are `r pval(fligner_price)` for the price,
`r pval(fligner_age)` for the age and `r pval(fligner_int)` combining both.
In the case of `age` and `age:fuelType`, results show that there is clear
evidence to reject the null hypothesis of equal variances for all groups.
The results when grouping by `fuelType` are more inconclusive as the *p.value*
is slightly over significance level.

<!-- 6. --> \pagebreak
## Calculate the linear regression model that explains the price from the age: interpret the regression line and assess its quality.

```{r q6_model}

lm_tibble <- function(lm_model) {
    summary_lm <- summary(lm_model)
    with(summary_lm, tribble(
        ~statistic, ~value,
        "Residual standard error", sigma,
        "Degrees of freedom", df,
        "Multiple R-squared", r.squared,
        "Adjusted R-squared", adj.r.squared,
        "F-statistic", fstatistic,
    ))
}

model1 <- lm(price ~ age, data = df)
model1 %>%
    xtable() %>%
    kable(
        caption = "Linear regression on price $\\sim$ age",
        booktabs = TRUE,
        digits = 2
    ) %>%
    kable_styling(latex_options = c("HOLD_position"), full_width = FALSE)
```

```{r q6_stats}
lm_tibble(model1) %>%
    kable(
        caption = "Linear regression on price $\\sim$ age statistics",
        booktabs = TRUE,
        digits = 2
    ) %>%
    kable_styling(latex_options = c("HOLD_position"), full_width = FALSE)

```

```{r q6_plot, fig.height = 4, fig.width=6, fig.cap = "Linear model price $\\sim$ age"}
df %>%
    ggplot(aes(age, price)) +
    geom_boxplot(aes(group = age)) +
    geom_smooth(method = "lm", formula = "y ~ x") +
    labs(title = paste(
        "Adj R2 = ", signif(summary(model)$adj.r.squared, 5),
        "Intercept =", signif(model$coef[[1]], 5),
        " Slope =", signif(model$coef[[2]], 5),
        " P =", signif(summary(model)$coef[2, 4], 5)
    ))
```

```{r q6_resid, fig.height = 6, fig.width = 8, fig.cap = "Model residuals"}
autoplot(model1)
```

\Cref{fig:q6_plot} shows the linear regression model on `price ~ age`. The
model parameters and statistics is shown in \cref{tab:q6_model,tab:q6_stats}.
A simple visual analysis shows that around 10 years, the `price` in our model
goes negative, which does not make sense in the real world. The fit is clearly
skewed by the larger amount of data with lower age values. In general this is a
very bad fit.

```{r q6_bptest}
(q6_bp <- bptest(model1))
```

In \cref{fig:q6_resid} we can see that the residuals seem to not hold
homoskedasticity. Performing a Breusch-Pagan Test we see that the _p-value_
is `r pval(q6_bp)` which is less than 0.05 and thus we reject the NULL hypothesis
of homoskedasticity. The results of the test are consistent with what is shown
in the residuals from \cref{fig:q6_plot} which seem to suggest both
no-linearity and heteroscedasticity.

<!-- 7. --> \pagebreak
## What is the percentage of the price variability that is explained by the age of the car?

```{r}
summary(model1)
```

The age explains `r summary(model1)$adj.r.squared` of the price variability
according to our model.



<!-- 8. --> \pagebreak
## Do you think it is necessary to introduce a quadratic term in the equation that relates the price to its age?

```{r q8, fig.height = 6, fig.width = 8, fig.cap = "Residuals of model with quadratic term" }
model2 <- lm(price ~ poly(age, 2), df)
summary(model2)

(anova <- anova(model1, model2))
autoplot(model2, ncol = 2)
```

The new model explains `r summary(model2)$adj.r.squared` of the price variance which
is an improvement from the previous one. Moreover the quadratic age term seems to
be relevant because when testing if its coefficient is equal to zero we get a really
small *p.value*.

Additionally, we compared the previous model with the new one which adds the
quadratic term using ANOVA. The resulting small *p.value* of `r pval(anova)`
makes us reject the null hypothesis of equal means. This implies that the new
model significantly improves on the previous one.

```{r q8_bptest}
(q8_bp <- bptest(model2))
```

Nevertheless, there is still a clear pattern of heteroscedasticity as seen in
\cref{fig:q8} and that is statistically proved through the Breusch-Pagan test
(with _p-value_ < 0.05 we reject the null hypothesis of homoscedasticity).




<!-- 9. --> \pagebreak
## Are there any additional explanatory numeric variables needed to the car price? Study collinearity effects.

```{r q9_models}
model_add <- lm(price ~ ., data = select(df, where(is.numeric)))
model_add_nomil <- lm(price ~ ., data = select(df, where(is.numeric), -mileage))

summary(model_add)
```

```{=latex }
\begin{multicols}{2}
```

```{r q9_vif}
vif(model_add) %>%
    kable(
        caption = "Variance inflation factors",
        booktabs = TRUE,
        digits = 2,
        col.names = c("VIF")
    ) %>%
    kable_styling(latex_options = c("HOLD_position"), full_width = FALSE)
```

```{r q9_vif_nomil}
vif(model_add_nomil) %>%
    kable(
        caption = "Variance inflation factors (without mileage)",
        booktabs = TRUE,
        digits = 2,
        col.names = c("VIF")
    ) %>%
    kable_styling(latex_options = c("HOLD_position"), full_width = FALSE)
```

```{=latex }
\end{multicols}
```

Performing a variance inflation factor analysis, we see that both `age` and
`mileage` have high values, if we examine their correlation, we can see that it
seems to be a linear correlation between the two, as shown in \cref{fig:q9_am}.
Additionally, `mileage` has the biggest _p-value_ in our model. Given these two
facts, consider not using `mileage` in the model.

```{r q9_am, fig.cap = "Colinearity between age and mileage"}
df %>%
    ggplot(aes(age, mileage)) +
    geom_boxplot(aes(as_factor(age))) +
    geom_smooth(method = "lm", formula = "y ~ x")
```

The new model without `mileage` has almost the same `R-squared` value and the
results of the `VIF` analysis are much more reasonable.
There seems to be a small correlation between `tax` and `mpg`, but it is not
significantly relevant as shown by the small `VIF` values.

```{r q9_nomil}
summary(model_add_nomil)
```

<!-- 10. --> \pagebreak
## After controlling by numerical variables, indicate whether the additive effect of the available factors on the price are statistically significant.

```{r}
m0 <- lm(price ~ poly(age, 2) + tax + mpg + mileage, data = df)
m1 <- lm(price ~ poly(age, 2) + tax + mpg + mileage + model, data = df)
m2 <- lm(price ~ poly(age, 2) + tax + mpg + mileage + manufacturer, data = df)
m3 <- lm(price ~ poly(age, 2) + tax + mpg + mileage + transmission, data = df)
m4 <- lm(price ~ poly(age, 2) + tax + mpg + mileage + fuelType, data = df)
m5 <- lm(price ~ poly(age, 2) + tax + mpg + mileage + engineSize, data = df)

anova(m0, m1)
anova(m0, m2)
anova(m0, m3)
anova(m0, m4)
anova(m0, m5)
```

Performing an analysis of covariance between the model and all the available
factors, we obtain that the additive effect of each of them on the `price` is
statistically significant (_p-value_ is less than `r "$2.2\\times10^{-16}$"` in
all the cases).

<!-- 11. --> \pagebreak
## Select the best model available so far. Interpret the equations that relate the explanatory variables to the answer (rate).

So far, the best model obtained so far is the one which includes all the
numerical variables and the quadratic factor on `age`.

```{r}
model_so_far <- lm(price ~ . + I(age^2), data = select(df, where(is.numeric)))
model_so_far$coefficients %>%
    kable(
        caption = "Model coefficients",
        booktabs = TRUE,
        digits = 2,
        col.names = c("Coefficient")
    ) %>%
    kable_styling(latex_options = c("HOLD_position"), full_width = FALSE)
```

The Intercept shows us that the expected initial value for a new car is
around 32000 pounds. For every mile the price drops
by `r abs(model_so_far$coefficients[2])`. For each pound of the tax, the value
increases by `r abs(model_so_far$coefficients[3])`. Contrary to what one
might think, miles per gallon (`mpg`) has a negative effect on the price of the
car, this may be caused by the extreme outliers in the `mpg` variable which are
the _BMW - i3_, a very expensive car with hybrid technology that uses petrol
to charge the electric batteries and extend its range.

The price of the car drops by `r abs(model_so_far$coefficients[4])` for each
year of age. Note that with this slope, at around 10 years, the `price` would
be negative, this is compensated by the `r "$age^2$"` factor. However, this
means that the model does not translate well to cars much older than the ones
in our sample, since the `r "$age^2$"` increases more rapidly than `age` meaning
that there is a point where the car price starts to increase the older it gets.
This may be valid in some cases with vintage cars, but in general common sense
dictates that it should approach a base value close to 0 as `age` tends to
infinity.


<!-- 12. --> \pagebreak
## Study the model that relates the logarithm of the price to the numerical variables.

```{r q12_boxcox, fig.cap = "log-Likelihood plot"}
dd <- df %>% select(-year, -starts_with("aux_"))
model_bc <- lm(price ~ . + I(age^2), data = dd)
bc <- boxcox(model_bc)
(lambda <- bc$x[which.max(bc$y)])
```

If we compute the \ensuremath{\lambda} value of the boxcox transformation as
shown in \cref{fig:q12_boxcox}, we obtain a value of lambda of `r lambda`. The
graphic shows that 0 is inside our confidence interval, indicating that a
log transformation of the data is needed.

```{r q12_log_plot, fig.height = 6, fig.width = 8, fig.cap = "Model residuals"}
log_model <- lm(log(price) ~ . + I(age^2), data = select(df, where(is.numeric)))
(sum_log <- summary(log_model))
autoplot(log_model, ncol = 2)
```

With the logarithm of the price, we obtain a higher value of `r glue("$R^2={signif(sum_log$adj.r.squared, 6)}$")`.
The residuals of the model are shown in \cref{fig:q12_log_plot}.

<!-- 13. --> \pagebreak
## Once explanatory numerical variables are included in the model, are there any main effects from factors needed?

```{r}
m5_model <- lm(log(price) ~ ., data = select(df, -year, -starts_with("aux_")))
m5 <- lm(log(price) ~ ., data = select(df, -year, -model, -starts_with("aux_")))
anova(m5_model, test = "Chisq")
```

If we add all factor variables to the model (except auxiliary ones), we obtain a
model which explains `r summary(m5_model)$r.squared` of the variability. The
factor variable `model` has great influence, if we remove it our model
covers `r summary(m5)$r.squared`. This makes sense, given that we expect cars
from the same model to have similar prices, also `model` is by far the category
with most factors.

<!-- 14. --> \pagebreak
## Graphically assess the best model obtained so far.

The best model obtained so far is the one using the log transformation on price
and all the numerical variables and factors.

```{r}
best_model <- lm(log(price) ~ ., data = select(df, -year, -starts_with("aux_")))
best_model_nomil <- lm(log(price) ~ ., data = select(df, -year, -starts_with("aux_"), -mileage))
best_model_notax <- lm(log(price) ~ ., data = select(df, -year, -starts_with("aux_"), -tax))
best_model_nomil_notax <- lm(log(price) ~ ., data = select(df, -year, -starts_with("aux_"), -mileage, -tax))

s_bm <- summary(best_model)
s_bm_nomil <- summary(best_model_nomil)
s_bm_notax <- summary(best_model_notax)
s_bm_nomil_notax <- summary(best_model_nomil_notax)

bic <- BIC(best_model)
bic_nomil <- BIC(best_model_nomil)
bic_notax <- BIC(best_model_notax)
bic_nomil_notax <- BIC(best_model_nomil_notax)
```

Given what we found on about the
collinearity of `mileage` and `age` we consider the model without including
`mileage`. Also we found that the significance level of the numeric variable
`tax` is not statistically significant when all the factors are added. We
evaluated the \ensuremath{R^2} and `BIC` of the model, the model without
`mileage`, without `tax` and without `mileage` or `tax`. The results are shown
in \cref{tab:q14_models}. The best model is the original one with
both `tax` and `mileage` (has higher \ensuremath{R^2} and lower `BIC`).

```{r q14_models}
tribble(
        ~model, ~R2, ~BIC,
        "base", s_bm$adj.r.squared, bic,
        "-mileage", s_bm_nomil$adj.r.squared, bic_nomil,
        "-tax", s_bm_notax$adj.r.squared, bic_notax,
        "-mileage, -tax", s_bm_nomil_notax$adj.r.squared, bic_nomil_notax
) %>%
    kable(
        caption = "R2 and BIC for log(price) model variations",
        booktabs = TRUE,
        digits = 4
    ) %>%
    kable_styling(latex_options = c("HOLD_position"), full_width = FALSE)
```

\Cref{fig:q14_plot} shows the residuals of the best model obtained so far. We
can see that there are no clear patterns on the residuals. There are still some
residuals which are clearly outliers and the distribution of residuals in the
`qqplot` is highly tailed.

```{r q14_plot, fig.height = 6, fig.width = 8, fig.cap = "Residuals of log(price) model", warning = FALSE}
autoplot(best_model, which = c(1, 2, 3, 6))
```

<!-- 15. --> \pagebreak
## Assess the presence of outliers in the studentized residuals at a 99% confidence level. Indicate what those observations are.

```{r, fig.height = 5, fig.width = 8, fib.cap = "Studentized residuals outliers", warning = FALSE}
df %>%
    add_column(stud_resids = studres(best_model)) %>%
    mutate(stud_outlier = is_severe_outlier(stud_resids)) %>%
    rownames_to_column("observation") %>%
    mutate(label = ifelse(stud_outlier, observation, "")) %>%
    ggplot(aes(age, stud_resids, color = stud_outlier)) +
    geom_jitter() +
    geom_label_repel(aes(label = label)) +
    geom_hline(yintercept = 0, color = "black") +
    labs(y = "Studentized residuals", color = "Outlier")
```


<!-- 16. --> \pagebreak
## Study the presence of a priori influential data observations, indicating their number according to the criteria studied in class.

In the initial analysis of the data, we identified `r nrow(df_outliers)`
multivariate outliers using Mahalanobis distance. The list of all the indices is
shown below:

```{r}
(id_apriori <- df_outliers %>% rowid_to_column() %>% filter(moutlier) %>% pull(rowid))
```


<!-- 17. --> \pagebreak
## Study the presence of a posteriori influential values, indicating the criteria studied in class and the actual atypical observations.

```{r}
col_num <- df %>% select(where(is.numeric)) %>% colnames()

dfb <- dfbetas(best_model) %>%
    as.data.frame() %>%
    select(any_of(col_num)) %>%
    rename_with(~ paste0("dfbetas_", .))

cook_dffit <- tibble(
        dffit = dffits(best_model),
        cooks.distance = cooks.distance(best_model)
    )

influential <- cbind(dfb, cook_dffit) %>%
    tibble() %>%
    rowid_to_column("observation")

ids_dfbetas <- influential %>%
    filter(if_any(c(-observation, -dffit), ~ abs(.) > 0.5)) %>%
    pull(observation)

ids_dffit <- influential %>%
    filter(if_any(dffit, ~ abs(.) > 1.0)) %>%
    pull(observation)

ids_combined <- union(ids_dfbetas, ids_dffit)

id_nopriori <- setdiff(ids_combined, id_apriori)
```

\Cref{fig:dfbetas_plot} shows the plot of the influential data using `DFBETAS`
for the different numerical variables as well as cooks distance. Since we have a
big sample of 5000 observations, we used the cutoff at 0.5.

```{r dfbetas_plot, fig.cap = "Influential data with DFBETAS", fig.width = 8, fig.height = 6, warning = FALSE}
influential %>%
    select(-dffit) %>%
    pivot_longer(-observation) %>%
    mutate(label = ifelse(abs(value) > 0.5, observation, "")) %>%
    ggplot(aes(observation, value, color = name)) +
    geom_line() +
    geom_hline(yintercept = 0) +
    geom_hline(yintercept = c(0.5, -0.5), linetype = "dotted") +
    geom_hline(yintercept = c(1, -1), linetype = "dashed") +
    geom_label_repel(aes(label = label)) +
    labs(color = "Metric")
```

The plot in \cref{fig:diffit_plot} shows the `DFFIT` metric for the different
observations in the dataset. The labels shown correspond to the values above 1.
We can see that most of the influential values found using `DFBETAS` are also influential
using `DFFIT`.

```{r diffit_plot, fig.cap = "Influential data with DFFIT", fig.width = 8, fig.height = 6, warning = FALSE}
influential %>%
    mutate(label = ifelse(abs(dffit) > 1.0, observation, "")) %>%
    ggplot(aes(observation, dffit)) +
    geom_segment(aes(xend = observation, yend = 0)) +
    geom_hline(yintercept = c(0.5, -0.5), linetype = "dotted", color = "blue") +
    geom_hline(yintercept = c(1, -1), linetype = "dashed", color = "blue") +
    geom_label_repel(aes(label = label)) +
    labs(y = "DFFIT")
```

```{r q17_outliers}
df %>%
    rowid_to_column() %>%
    select(-starts_with("aux_"), -manufacturer) %>%
    add_column(Moutlier = df_outliers$moutlier) %>%
    mutate(Moutlier = cell_spec(Moutlier, bold = Moutlier)) %>%
    slice(ids_combined) %>%
    arrange(rowid) %>%
    kable(
        caption = "Influential data",
        booktabs = TRUE,
        digits = 2,
        escape = FALSE
    ) %>%
    kable_styling(latex_options = c("HOLD_position", "scale_down"), full_width = FALSE)
```

In \cref{tab:q17_outliers} we show all the influential data labelled with either
`DFFIT` or `DFBETAS`. The column `Moutlier` shows the variables which where
labelled as multivariate outliers _a priori_. In more than half the cases the
influential data was not a multivariate outlier we detected _a priori_.


<!-- 18. --> \pagebreak
## Given a 5-year-old car, the rest of numerical variables on the mean and factors on the reference level, what would be the expected price at 95% confidence interval?

We use the model from previous sections but removing all the influential data
points found in the previous section.

```{r q18_model}
dd <- df %>%
    select(-year, -starts_with("aux_")) %>%
    slice(-ids_combined)

best_model_noinf <- lm(log(price) ~ ., data = dd)
```

```{r q18_car_data}
df_mean <- df %>%
    summarise(across(c(where(is.numeric), -age), mean))

car_5y <- df %>%
    slice(1) %>%
    select(where(is.factor), -year, -starts_with("aux_")) %>%
    bind_cols(df_mean) %>%
    add_column(age = 5)
```

```{r q18_car_data_table}
car_5y %>%
    select(-price) %>%
    kable(
        caption = "5 year old car with mean numerical variables and reference level on factors",
        booktabs = TRUE,
        digits = 2
    ) %>%
    kable_styling(latex_options = c("HOLD_position", "scale_down"), full_width = FALSE)
```

```{r q18_car_data_pred}
(predicted <- best_model_noinf %>%
    predict(newdata = car_5y, interval = "confidence", level = 0.95) %>%
    exp() %>%
    round(2) %>%
    as.data.frame())
```

For the data shown in \cref{tab:q18_car_data_table} we obtain an expected `price`
of `r glue("{predicted$fit}")` with a 95% confidence interval
of `r glue("({predicted$lwr}, {predicted$upr})")`.

<!-- 19. --> \pagebreak
## Summarize what you have learned by working with this interesting real dataset.
