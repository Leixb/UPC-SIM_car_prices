---
output:
  pdf_document: default
  html_document: default
---
```{r, libraries_question, include=FALSE}
library(tidyverse)
library(cowplot)
library(corrplot)
library(car)
library(FactoMineR)
library(lmtest)
library(xtable)
library(ggfortify)
library(MASS)

select <- dplyr::select
recode <- dplyr::recode
some <- purrr::some
```

# Analysis
```{r}
source("io_checksum.R")
df <- read_rds_w_checksum("./data/dataset.rds")
```


<!-- 1 -->
## Determine if the response variable (price) has an acceptably normal distribution. Address test to discard serial correlation.

```{r q1_price_hist, echo = FALSE, fig.height = 3}

df <- df %>% mutate(price_log = log(price))

price_mean <- mean(df$price)
price_sd <- sd(df$price)

ggplot(df, aes(price)) +
    geom_histogram(aes(y = ..density..), bins = 30) +
    stat_function(
        fun = dnorm,
        args = list(mean = price_mean, sd = price_sd),
        aes(color = "normal"),
    ) +
    geom_density(aes(color = "density"), linetype = "dashed") +
    geom_text(aes(
        x = max(df$price) * 3 / 4, y = 1e-5,
        color = "normal"
    ),
    label = sprintf(
        "x̄ = %0.2f\ns = %0.2f",
        price_mean,
        price_sd
    ),
    show.legend = FALSE,
    ) +
    guides(color = guide_legend(title = NULL))
```

```{r q1_price_log_hist, echo = FALSE, fig.height = 3}

price_log_mean <- mean(df$price_log)
price_log_sd <- sd(df$price_log)

ggplot(df, aes(price_log)) +
    geom_histogram(aes(y = ..density..), bins = 30) +
    stat_function(
        fun = dnorm,
        args = list(mean = price_log_mean, sd = price_log_sd),
        aes(color = "normal")
    ) +
    geom_density(aes(color = "density"), linetype = "dashed") +
    geom_text(aes(
        x = 11, y = 0.75,
        color = "normal"
    ),
    label = sprintf(
        "x̄ = %0.2f\ns = %0.2f",
        price_log_mean,
        price_log_sd
    ),
    show.legend = FALSE,
    ) +
    guides(color = guide_legend(title = NULL))
```

```{r q1_shapiro}
(shap_res <- shapiro.test(df$price))
(log_shap_res <- shapiro.test(df$price_log))
```

The histogram of the price shows a very right skewed distribution which does not
seem compatible with a normal fit. Moreover, the Shapiro test returns a small
*p-value* of `r shap_res$p.value`, which makes us reject the null hypothesis
of normality.

Additionally, we also checked if the price followed a log normal distribution. If we
analyse the new histogram we can see that the log transformation corrected the skewness
and the new distribution seems to resemble a normal bell shape. However the Shapiro test
returns a similarly low *p-value* of `r log_shap_res$p.value`, which makes us
reject the null hypothesis of normality.

\Cref{fig:q1_qq_plots} shows the QQ plots of `price` and `log*(price)`. We can
see that `price` clearly does not follow a normal distribution and that
`log(price)` is heavy tailed.


```{r q1_qq_plots, echo=FALSE, fig.cap="QQ plots", fig.width=6}
plt_qq_price <- ggplot(df, aes(sample = price)) +
    stat_qq() +
    stat_qq_line(color = "red") +
    theme(aspect.ratio = 1) +
    labs(x = "Theoretical", y = "price")
plt_qq_log_price <- ggplot(df, aes(sample = log(price))) +
    stat_qq() +
    stat_qq_line(color = "red") +
    theme(aspect.ratio = 1) +
    labs(x = "Theoretical", y = "log(price)")

plot_grid(plt_qq_price, plt_qq_log_price, align = "hv", labels = "AUTO")
```

```{r}
dwtest_res <- dwtest(price ~ 1, data = df)
```

We perform a Durbin-Watson test with the null hypothesis that the
autocorrelation of the disturbances is 0. We obtain a *p-value* of
`r round(dwtest_res$p.value, 2)` so we fail to reject the null hypothesis.

```{r}
dwtest_res
```

The results of the test are consistent with the visual interpretation of
the ACF plot^[lag 0 is omitted for clarity] shown in \cref{fig:acf}.
All the values except lag = 33 lie within the confidence interval of 95%,
showing that there is no autocorrelation.

```{r acf, fig.cap = "ACF plot for price"}
acf_res <- acf(df$price, plot = FALSE)
acf_res_df <- with(acf_res, data.frame(lag, acf)) %>%
    slice(-1) # Remove lag 0

# Confidence interval
acf_ci <- function(acf_res, alpha = 0.05) {
    return(qnorm((1 + (1 - alpha)) / 2) / sqrt(acf_res$n.used))
}

conf_inter <- acf_ci(acf_res, 0.05)

ggplot(data = acf_res_df, mapping = aes(x = lag, y = acf)) +
    geom_hline(aes(yintercept = 0)) +
    geom_segment(mapping = aes(xend = lag, yend = 0)) +
    geom_hline(aes(yintercept = conf_inter), linetype = "dashed", color = "blue") +
    geom_hline(aes(yintercept = -conf_inter), linetype = "dashed", color = "blue") +
    labs(x = "Lag", y = "ACF")
```

<!-- 2. --> \pagebreak
## Indicate by exploration of the data, which are apparently the variables most associated with the response variable (use only the indicated variables).

Since we determined that `price` does not follow a normal distribution, we
compute a correlation matrix using the `spearman` coefficient. The plot
of the correlation is shown in \cref{fig:q2_corrplot} and shows that the
numerical variables most associated with price are: `age`, `mileage` and
`mpg`. Surprisingly, `tax` has the lowest correlation coefficient. The
specific values of the correlation matrix are shown in \cref{tab:q2_corr}.

is a clear

```{r q2_corrplot, fig.cap = "Spearman correlation plot", fig.height = 2.5}
cor_mat <- df %>%
    select(where(is.numeric), -price_log) %>%
    cor(use = "complete.obs", method = "spearman")

cor_mat %>% corrplot()
```

```{r q2_corr}
cor_mat %>%
    kable(
        caption = "Spearman correlation coefficients",
        booktabs = TRUE,
        digits = 2
    ) %>%
    kable_styling(latex_options = c("HOLD_position"), full_width = FALSE)
```

Using `condes` method from `FactoMineR`, we computed the correlation with the
qualitative variables. The most relevant qualitative variable is `model`,
closely followed by `engineSize` and then `year` (this correlates with the
results of the numerical variable `age`). Finally, `transmission` has little
less significance and `manufacturer` and `fuelType` have almost no significance.

```{r q2_factors}
df_cond <- df %>%
    select(-c(starts_with("aux_"))) %>%
    condes(which(colnames(.) == "price"))

df_cond$quali %>%
    data.frame() %>%
    rownames_to_column("Variable") %>%
    select(-p.value) %>%
    kable(
        caption = "Qualitative variable correlation with price",
        booktabs = TRUE,
        digits = 2
    ) %>%
    kable_styling(latex_options = c("HOLD_position"), full_width = FALSE)
```

The variables most associated with our response variable are (in decreasing
order of importance):

- `model`
- `engineSize`
- `year` / `age`
- `mileage`
- `mpg`

<!-- 3. --> \pagebreak
## Define a polytomic factor f.age for the covariate car age according to its quartiles, and argue if the average price depends on the level of age.  Statistically justify the answer.

We start by checking the ANOVA assumptions of normality and homogeneity of
variance.

```{r anova_assumptions, fig.cap = "Boxplot of price by age group"}
df %>%
    ggplot(aes(x = aux_age, y = price, fill = aux_age)) +
    geom_boxplot()

# Homogenity of variance
flig_res <- fligner.test(price ~ aux_age, data = df); flig_res

# Normality
model <- lm(price ~ aux_age, data = df)
shapiro.test(residuals(model))
```
The fligner test returns a *p-value* of `r flig_res$p.value` which makes us reject the
null hypothesis of homoscedasticity. Additionally, we can't assume normality. For this
reason we will use the non-parametric Kruskal-Wallis test.

```{r q3_bp}
(kruskal_res <- kruskal.test(price ~ aux_age, data = df))
```
The test returns a *p.value* of `r kruskal_res$p.value`. Which is less than
the significance level and thus we reject the NULL hypothesis that the location
parameters of all the samples are equal. We have statistical proof that
the average `price` does depend on the `age`. The visual inspection of
the boxplots in \cref{fig:anova_assumptions} is consistent with the results.

<!-- 4. --> \pagebreak
## Calculate and interpret the anova model that explains car price according to the age factor and the fuel type.

```{r q4}
df %>%
    ggplot(aes(x = aux_age, y = price, fill = fuelType)) +
    geom_boxplot()

anova <- aov(price ~ aux_age * fuelType, data = df)
summary(anova)
```
Anova results show that the both factors are significant (_p.value < 0.05_) as
well as their interaction.

```{r q4_summary}
df %>%
    group_by(aux_age, fuelType) %>%
    summarise(
        count = n(),
        mean = mean(price),
        sd = sd(price),
        .groups = "keep"
    ) %>%
    kable(
        caption = "Summary of price by age and fuelType",
        booktabs = TRUE,
        digits = 2
    ) %>%
    kable_styling(latex_options = c("HOLD_position"), full_width = FALSE)

```


<!-- 5. --> \pagebreak
## Do you think that the variability of the price depends on both factors? Does the relation between price and age factor depend on fuel type?

```{r}
(fligner_price <- fligner.test(price ~ fuelType, data = df))
(fligner_age <- fligner.test(price ~ aux_age, data = df))
(fligner_int <- fligner.test(price ~ interaction(aux_age, fuelType), data = df))
```
We execute Fligner-Killeen test with each factor and the interaction of both.
The resulting *p.values* are `r fligner_price$p.value` for the price,
`r fligner_age$p.value` for the age and `r fligner_int$p.value` combining both.
In the case of `age` and `age:fuelType`, results show that there is clear
evidence to reject the null hypothesis of equal variances for all groups. The
results when grouping by `fuelType` are more inconclusive as the *p.value* is
slightly over significance level.

<!-- 6. --> \pagebreak
## Calculate the linear regression model that explains the price from the age: interpret the regression line and assess its quality.

```{r q6_model}

lm_tibble <- function(lm_model) {
    summary_lm <- summary(lm_model)
    with(summary_lm, tribble(
        ~statistic, ~value,
        "Residual standard error", sigma,
        "Degrees of freedom", df,
        "Multiple R-squared", r.squared,
        "Adjusted R-squared", adj.r.squared,
        "F-statistic", fstatistic,
    ))
}

model1 <- lm(price ~ age, data = df)
model1 %>%
    xtable() %>%
    kable(
        caption = "Linear regression on price $\\sim$ age",
        booktabs = TRUE,
        digits = 2
    ) %>%
    kable_styling(latex_options = c("HOLD_position"), full_width = FALSE)
```

```{r q6_stats}
lm_tibble(model) %>%
    kable(
        caption = "Linear regression on price $\\sim$ age statistics",
        booktabs = TRUE,
        digits = 2
    ) %>%
    kable_styling(latex_options = c("HOLD_position"), full_width = FALSE)

```

```{r q6_plot, fig.height = 4, fig.width=6, fig.cap = "Linear model price ~ age"}
df %>%
    ggplot(aes(age, price)) +
    geom_boxplot(aes(group = age)) +
    geom_smooth(method = "lm", formula = "y ~ x") +
    labs(title = paste(
        "Adj R2 = ", signif(summary(model)$adj.r.squared, 5),
        "Intercept =", signif(model$coef[[1]], 5),
        " Slope =", signif(model$coef[[2]], 5),
        " P =", signif(summary(model)$coef[2, 4], 5)
    ))
```

```{r q6_resid, fig.height = 6, fig.width = 8, fig.cap = "Model residuals"}
autoplot(model1)
```

\Cref{fig:q6_plot} shows the linear regression model on `price ~ age`. The
relevant data is shown in \cref{tab:q6_model,tab:q6_stats}.

```{r q6_bptest}
(q6_bp <- bptest(model1))
```

In \cref{fig:q6_resid} we can see that the residuals seem to not hold
homoskedasticity. Performing a Breusch-Pagan Test we see that the _p-value_
is `r q6_bp$p.value` which is less than 0.05 and thus we reject the NULL hypothesis
of homoskedasticity.

<!-- 7. --> \pagebreak
## What is the percentage of the price variability that is explained by the age of the car?

```{r}
summary(model1)
```
The age explains `r summary(model1)$adj.r.squared` of the price variability.



<!-- 8. --> \pagebreak
## Do you think it is necessary to introduce a quadratic term in the equation that relates the price to its age?

```{r, fig.height = 6, fig.width = 8}
model2 <- lm(price ~ age + I(age^2), df)
summary(model2)

(anova <- anova(model1, model2))
autoplot(model2, ncol = 2)
```

The new model explains `r summary(model2)$adj.r.squared` of the price variance which
is an improvement from the previous one. Moreover the quadratic age term seems to
be relevant because when testing if its coefficient is equal to zero we get a really
small *p.value*.

Moreover, we compare the previous model with the new one which adds the quadratic term
using ANOVA. The resulting small *p.value* of `r anova$p.value` makes us reject the
null hypothesis of equal means. This implies that the new model significantly improves
on the previous one.



<!-- 9. --> \pagebreak
## Are there any additional explanatory numeric variables needed to the car price? Study collinearity effects.

```{r}
lm <- lm(price ~ ., data = select(df, where(is.numeric)))
summary(lm)

# Collinearity
df %>%
    select(where(is.numeric)) %>%
    cor(use = "complete.obs") %>%
    corrplot()

lm_mileage <- lm(mileage ~ . - price, select(df, where(is.numeric)))
summary(lm_mileage)
lm_tax <- lm(tax ~ . - price, select(df, where(is.numeric)))
summary(lm_tax)
lm_mpg <- lm(mpg ~ . - price, select(df, where(is.numeric)))
summary(lm_mpg)
lm_age <- lm(age ~ . - price, select(df, where(is.numeric)))
summary(lm_age)
```


<!-- 10. --> \pagebreak
## After controlling by numerical variables, indicate whether the additive effect of the available factors on the price are statistically significant.

```{r}
log_model <- lm(price ~ ., data = select(df, -price_log & where(is.numeric)))
log_model
```


<!-- 11. --> \pagebreak
## Select the best model available so far. Interpret the equations that relate the explanatory variables to the answer (rate).

<!-- 12. --> \pagebreak
## Study the model that relates the logarithm of the price to the numerical variables.

```{r, fig.height = 6, fig.width = 8}
log_model <- lm(price_log ~ ., data = select(df, -price & where(is.numeric)))
summary(log_model)
autoplot(log_model, ncol = 2)
```


<!-- 13. --> \pagebreak
## Once explanatory numerical variables are included in the model, are there any main effects from factors needed?

```{r}
m5 <- lm(price_log ~ ., data = select(df, -price))
anova(m5)
```


<!-- 14. --> \pagebreak
## Graphically assess the best model obtained so far.

```{r}
best_model <- m5
```


<!-- 15. --> \pagebreak
## Assess the presence of outliers in the studentized residuals at a 99% confidence level. Indicate what those observations are.

```{r}
stud_resids <- studres(best_model)

plot(df$age, stud_resids)
abline(0, 0)

df <- df %>%
    add_column(stud_resids = studres(best_model)) %>%
    mutate(stud_outlier = is_severe_outlier(stud_resids))

df %>%
    ggplot(aes(age, stud_resids, color = stud_outlier)) +
    geom_jitter() +
    geom_hline(yintercept = 0, color = "red")
```


<!-- 16. --> \pagebreak
## Study the presence of a priori influential data observations, indicating their number according to the criteria studied in class.

<!-- 17. --> \pagebreak
## Study the presence of a posteriori influential values, indicating the criteria studied in class and the actual atypical observations.

<!-- 18. --> \pagebreak
## Given a 5-year-old car, the rest of numerical variables on the mean and factors on the reference

<!-- 19. --> \pagebreak
## Summarize what you have learned by working with this interesting real dataset.
